{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Annotation Results - Analysis\n",
    "\n",
    "Explore and analyze reasoning traces generated by **Qwen2.5-72B-Instruct** teacher model.\n",
    "\n",
    "**Dataset**: Conversational COVID-19 tweets (news headlines already filtered out)\n",
    "\n",
    "**Five-Task Reasoning Chain**:\n",
    "- **R1**: Syntactic Parsing (dependency analysis)\n",
    "- **R2**: Aspect Extraction (COVID-19 related aspects)\n",
    "- **R3**: Opinion Extraction (opinion expressions)\n",
    "- **R4**: Sentiment Classification (positive/negative/neutral)\n",
    "- **R5**: Emotion Classification (fear, anger, joy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the latest conversational annotation file\nannotation_files = glob.glob(\"data/COVIDSenti/COVIDSenti_conversational_annotated_*.csv\")\n\nif not annotation_files:\n    print(\"[ERROR] No annotation files found!\")\n    print(\"\\nRun annotation first:\")\n    print(\"  sbatch scripts/run_annotation.sh\")\nelse:\n    # Sort by file size (larger = more samples) to get latest\n    annotation_files.sort(key=lambda x: Path(x).stat().st_size, reverse=True)\n    file_path = annotation_files[0]\n    \n    df = pd.read_csv(file_path)\n    print(f\"Loaded: {file_path}\")\n    print(f\"\\nConversational tweets annotated: {len(df):,}\")\n    print(f\"Columns: {list(df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSample tweet:\")\n",
    "print(f\"  {df.iloc[0]['tweet'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare visualization\n",
    "label_counts = df['label'].value_counts()\n",
    "label_names = {'neu': 'Neutral', 'neg': 'Negative', 'pos': 'Positive'}\n",
    "label_colors = {'neu': '#95a5a6', 'neg': '#e74c3c', 'pos': '#2ecc71'}\n",
    "\n",
    "display_labels = [label_names.get(lbl, lbl) for lbl in label_counts.index]\n",
    "colors = [label_colors.get(lbl, '#3498db') for lbl in label_counts.index]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "label_counts.plot(kind='bar', ax=ax1, color=colors)\n",
    "ax1.set_title('Sentiment Distribution (Conversational Tweets)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Sentiment', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xticklabels(display_labels, rotation=0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "label_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=colors, \n",
    "                   labels=display_labels, startangle=90)\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_title('Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Annotations\n",
    "\n",
    "Explore individual reasoning traces for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def display_annotation(idx):\n    \"\"\"Display a formatted annotation for a given index.\"\"\"\n    if idx >= len(df):\n        print(f\"Index {idx} out of range (max: {len(df)-1})\")\n        return\n    \n    row = df.iloc[idx]\n    \n    output = f\"\"\"\n# Example {idx + 1} / {len(df)}\n\n## Tweet\n**\"{row['tweet']}\"**\n\n**Ground Truth Label:** `{row['label'].upper()}`\n\n---\n\n## R1: Syntactic Parsing\n{row['r1_syntactic']}\n\n---\n\n## R2: Aspect Extraction\n{row['r2_aspects']}\n\n---\n\n## R3: Opinion Extraction\n{row['r3_opinion']}\n\n---\n\n## R4: Sentiment Classification\n{row['r4_sentiment']}\n\n---\n\n## R5: Emotion Classification\n{row['r5_emotion']}\n\"\"\"\n    display(Markdown(output))\n\n# Display first example\ndisplay_annotation(0)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_idx = random.randint(0, len(df)-1)\n",
    "print(f\"Random example (index {random_idx}):\")\n",
    "display_annotation(random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Sentiment\n",
    "\n",
    "Browse examples by specific sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sentiment to explore\n",
    "sentiment = 'neg'  # Change to 'pos', 'neg', or 'neu'\n",
    "\n",
    "sentiment_df = df[df['label'] == sentiment]\n",
    "print(f\"Found {len(sentiment_df):,} tweets with '{sentiment.upper()}' sentiment\")\n",
    "print(f\"Showing first example:\\n\")\n",
    "\n",
    "if len(sentiment_df) > 0:\n",
    "    # Get the actual index in the original dataframe\n",
    "    original_idx = sentiment_df.index[0]\n",
    "    display_annotation(original_idx)\n",
    "else:\n",
    "    print(f\"No examples found with sentiment '{sentiment}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning Trace Statistics\n",
    "\n",
    "Analyze the length and characteristics of reasoning traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_cols = ['r1_syntactic', 'r2_aspects', 'r3_opinion', 'r4_sentiment', 'r5_emotion']\n",
    "task_names = ['R1: Syntactic', 'R2: Aspects', 'R3: Opinion', 'R4: Sentiment', 'R5: Emotion']\n",
    "\n",
    "# Calculate trace lengths\n",
    "lengths = []\n",
    "for col in reasoning_cols:\n",
    "    df[f'{col}_len'] = df[col].str.len()\n",
    "    lengths.append(df[f'{col}_len'].mean())\n",
    "\n",
    "print(\"Average reasoning trace lengths:\")\n",
    "print(\"=\" * 50)\n",
    "for name, col, length in zip(task_names, reasoning_cols, lengths):\n",
    "    print(f\"  {name:20s}: {length:6.0f} characters\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(task_names, lengths, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6'])\n",
    "ax.set_title('Average Reasoning Trace Length by Task', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Task', fontsize=12)\n",
    "ax.set_ylabel('Characters', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Samples\n",
    "\n",
    "Export a subset for manual inspection or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export 10 random examples for inspection\nsample_df = df.sample(n=min(10, len(df)), random_state=42)\noutput_path = \"data/COVIDSenti/sample_annotations_for_review.csv\"\nsample_df.to_csv(output_path, index=False)\nprint(f\"Exported {len(sample_df)} sample annotations to:\")\nprint(f\"  {output_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Generate More Annotations\n",
    "\n",
    "To annotate more conversational tweets:\n",
    "\n",
    "1. **Edit configuration** in `scripts/annotate.py`:\n",
    "   ```python\n",
    "   N_SAMPLES = 5000  # Your desired batch size\n",
    "   ```\n",
    "\n",
    "2. **Run pre-flight check**:\n",
    "   ```bash\n",
    "   python3 scripts/preflight_check.py\n",
    "   ```\n",
    "\n",
    "3. **Submit job**:\n",
    "   ```bash\n",
    "   sbatch scripts/run_annotation.sh\n",
    "   ```\n",
    "\n",
    "4. **Monitor**:\n",
    "   ```bash\n",
    "   squeue -u $USER\n",
    "   tail -f annotation_conversational_*.log\n",
    "   ```\n",
    "\n",
    "### Next Phase: Student Model Training\n",
    "\n",
    "Once you have sufficient annotations (recommended: 5,000+), proceed to:\n",
    "- Train LLaMA-3-8B student model using these reasoning traces\n",
    "- Use LoRA for parameter-efficient fine-tuning\n",
    "- Evaluate on held-out test set\n",
    "\n",
    "See `modeling/` directory for training scripts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}