{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Annotation Analysis\n",
    "\n",
    "Qwen2.5-72B-Instruct reasoning traces on COVID-19 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_files = glob.glob(\"data/COVIDSenti/COVIDSenti_conversational_annotated_*.csv\")\n",
    "\n",
    "if not annotation_files:\n",
    "    print(\"No annotation files found\")\n",
    "else:\n",
    "    annotation_files.sort(key=lambda x: Path(x).stat().st_size, reverse=True)\n",
    "    file_path = annotation_files[0]\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded: {file_path}\")\n",
    "    print(f\"Samples: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSample tweet:\")\n",
    "print(f\"  {df.iloc[0]['tweet'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "label_names = {'neu': 'Neutral', 'neg': 'Negative', 'pos': 'Positive'}\n",
    "label_colors = {'neu': '#95a5a6', 'neg': '#e74c3c', 'pos': '#2ecc71'}\n",
    "\n",
    "display_labels = [label_names.get(lbl, lbl) for lbl in label_counts.index]\n",
    "colors = [label_colors.get(lbl, '#3498db') for lbl in label_counts.index]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "label_counts.plot(kind='bar', ax=ax1, color=colors)\n",
    "ax1.set_title('Sentiment Distribution')\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticklabels(display_labels, rotation=0)\n",
    "\n",
    "label_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=colors, \n",
    "                   labels=display_labels, startangle=90)\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_title('Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotation(idx):\n",
    "    if idx >= len(df):\n",
    "        print(f\"Index {idx} out of range\")\n",
    "        return\n",
    "    \n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    output = f\"\"\"\n",
    "# Example {idx + 1} / {len(df)}\n",
    "\n",
    "## Tweet\n",
    "**\"{row['tweet']}\"**\n",
    "\n",
    "**Label:** `{row['label'].upper()}`\n",
    "\n",
    "---\n",
    "\n",
    "## R1: Syntactic Parsing\n",
    "{row['r1_syntactic']}\n",
    "\n",
    "---\n",
    "\n",
    "## R2: Aspect Extraction\n",
    "{row['r2_aspects']}\n",
    "\n",
    "---\n",
    "\n",
    "## R3: Opinion Extraction\n",
    "{row['r3_opinion']}\n",
    "\n",
    "---\n",
    "\n",
    "## R4: Sentiment Classification\n",
    "{row['r4_sentiment']}\n",
    "\n",
    "---\n",
    "\n",
    "## R5: Emotion Classification\n",
    "{row['r5_emotion']}\n",
    "\"\"\"\n",
    "    display(Markdown(output))\n",
    "\n",
    "display_annotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_idx = random.randint(0, len(df)-1)\n",
    "display_annotation(random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = 'neg' #neg pos or neu\n",
    "\n",
    "sentiment_df = df[df['label'] == sentiment]\n",
    "print(f\"Found {len(sentiment_df):,} tweets with '{sentiment.upper()}' sentiment\")\n",
    "\n",
    "if len(sentiment_df) > 0:\n",
    "    original_idx = sentiment_df.index[0]\n",
    "    display_annotation(original_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning Trace Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_cols = ['r1_syntactic', 'r2_aspects', 'r3_opinion', 'r4_sentiment', 'r5_emotion']\n",
    "task_names = ['R1: Syntactic', 'R2: Aspects', 'R3: Opinion', 'R4: Sentiment', 'R5: Emotion']\n",
    "\n",
    "lengths = []\n",
    "for col in reasoning_cols:\n",
    "    df[f'{col}_len'] = df[col].str.len()\n",
    "    lengths.append(df[f'{col}_len'].mean())\n",
    "\n",
    "print(\"Average reasoning trace lengths:\")\n",
    "for name, length in zip(task_names, lengths):\n",
    "    print(f\"  {name:20s}: {length:6.0f} characters\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(task_names, lengths, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6'])\n",
    "ax.set_title('Average Reasoning Trace Length by Task')\n",
    "ax.set_xlabel('Task')\n",
    "ax.set_ylabel('Characters')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(n=min(10, len(df)), random_state=42)\n",
    "output_path = \"data/COVIDSenti/sample_annotations_for_review.csv\"\n",
    "sample_df.to_csv(output_path, index=False)\n",
    "print(f\"Exported {len(sample_df)} samples to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aria_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
